---
title: MongoDB高可用方案之复制集
icon: pen-to-square
date: 2024-11-19
category:
  - MongoDB
tag:
  - NoSQL
  - Database
---

# MongoDB 高可用方案

## 目录

- [复制集](#复制集)
  - [高可用特性](#高可用特性)
  - [复制集结构](#复制集结构)
    - [PSS 模式（官方推荐）](#PSS模式官方推荐)
    - [PSA 模式](#PSA模式)
  - [复制集中的各个角色](#复制集中的各个角色)
    - [Primary → 主节点](#Primary--主节点)
    - [Secondary → 备节点](#Secondary--备节点)
    - [Arbiter → 总裁节点](#Arbiter--总裁节点)
  - [数据复制](#数据复制)
    - [oplog](#oplog)
  - [选举](#选举)
    - [选举原理](#选举原理)
    - [常见选项](#常见选项)
  - [自动故障转移](#自动故障转移)
  - [搭建 MongoDB 复制集](#搭建MongoDB复制集)
    - [创建数据目录](#创建数据目录)
    - [准备配置文件](#准备配置文件)
    - [开启服务](#开启服务)
    - [配置复制集](#配置复制集)
      - [方式一](#方式一)
      - [方式二](#方式二)
    - [查看状态](#查看状态)
    - [其他常用指令](#其他常用指令)
    - [复制集的连接方式](#复制集的连接方式)
  - [安全认证](#安全认证)
    - [连接主服务器](#连接主服务器)
    - [创建用户](#创建用户)
    - [停止服务](#停止服务)
    - [创建 keyFile 文件](#创建keyFile文件)
    - [启动服务](#启动服务)
    - [测试连接](#测试连接)

## 复制集

### 高可用特性

复制集高可用的方面体现在两个方面

1. 数据写入时将数据迅速复制到另一个独立节点上
2. 在接受写入的节点发生故障时自动选举出一个新的替代节点

### 复制集结构

#### PSS 模式（官方推荐）

官方推荐也是最为典型的一个模式 PSS 由 3 个以上具有投票权的节点组成，包括：

1. 主节点（PRIMARY）：接受写入操作和选举时投票
2. 两个（或多个）从节点（SECONDARY）：复制主节点上的新数据和选举时投票
3. 如果主节点不可用，则复制集选择备节点作为主节点并继续正常操作。旧的主节点在可用时重新加入复制集

#### PSA 模式

PSA 模式由一个主节点、一个备节点和一个仲裁者节点组成，Arbiter 节点不存储数据副本，也不提供业务的读写操作。Arbiter 节点发生故障不影响业务，仅影响选举投票

### 复制集中的各个角色

#### Primary → 主节点

主节点，其接收所有的写请求，然后把修改同步到所有备节点。一个复制集只能有一个主节点，当主节点“挂掉”后，其他节点会重新选举出来一个主节点

#### Secondary → 备节点

备节点，与主节点保持同样的数据集。当主节点“挂掉”时，参与竞选主节点。

#### Arbiter → 总裁节点

仲裁节点，只用于参与选举投票，本身不承载任何数据，只作为投票角色。

### 数据复制

当一个修改操作（插入，更新或删除）到达主节点时，他对数据的操作将会被记录下来，这部分记录下来的数据成为 oplog，从节点通过在主节点上打开一个 tailable 游标不断获取新进入主节点的 oplog，并在自己的数据上回放，以此保持跟主节点的数据一致

#### oplog

MongoDB oplog 是 Local 库下的一个集合，用来保存写操作所产生的**增量日志**，且是一个**固定集合**，固定集合意味着他的大小是一定的，如果数据量达到了这个大小，就会删掉最老的历史数据，主节点产生新的 oplog Entry，从节点通过复制 oplog 并应用来保持和主节点的状态一致，oplog 对数据的事务回滚，数据复制起着非常关键性的作用。

当然我们可以通过命令来查看 oplog

```mongodb
use local
db.oplog.rs.find().sort({$natural:-1}).pretty()
```

更多的 oplog 操作

```mongodb
local.system.replset：用来记录当前复制集的成员。
local.startup_log：用来记录本地数据库的启动日志信息。
local.replset.minvalid：用来记录复制集的跟踪信息，如初始化同步需要的字段。
ts： 操作时间，当前timestamp + 计数器，计数器每秒都被重置
v：oplog版本信息
op：操作类型:
i：插⼊操作
u：更新操作
d：删除操作
c：执行命令（如createDatabase，dropDatabase）
n：空操作，特殊用途
ns：操作针对的集合
o：操作内容
o2：操作查询条件，仅update操作包含该字段
```

### 选举

#### 选举原理

MongoDB 出现主节点故障，内部会通过选举的方式完成故障恢复，选举的方法是通过 Raft 算法来实现的

Raft 算法：[https://raft.github.io/](https://raft.github.io/ "https://raft.github.io/")

MongoDB 在 Raft 的标准算法上还加了一些自己的扩展：

- 支持 chainingAllowed 链式复制，即备节点不只是从主节点上同步数据，还可以选择一个离自己最近（心跳延时最小）的节点来复制数据。
- 增加了预投票阶段，即 preVote，这主要是用来避免网络分区时产生 Term(任期)值激增的问题
- 支持投票优先级，如果备节点发现自己的优先级比主节点高，则会主动发起投票并尝试成为新的主节点。

具体流程可分为

1. 具有投票权的节点之间两两互相发送心跳
2. 当 5 次心跳未收到时判断节点失联
3. 如果主节点失联，从节点会发起选举，选出新的主节点

   被选举为主节点的节点

   1. 能够与多数节点建立连接
   2. 具有较新的 oplog
   3. 具有较高的优先级

4. 如果失联的是从节点则不会产生新的选举
5. 选举基于 RAFT 一致性算法实现，选举成功的必要条件是大多数投票节点存活
6. 复制集中最多可以有 50 个节点，但具有投票权的节点最多 7 个，因为一旦过多的成员参与数据复制、投票过程，将会带来更多可靠性方面的问题
   | 投票成员数 | 大多数 | 容忍 |
   | ----- | --- | -- |
   | 1 | 1 | 0 |
   | 2 | 2 | 0 |
   | 3 | 2 | 1 |
   | 4 | 3 | 1 |
   | 5 | 3 | 2 |
   | 6 | 4 | 2 |
   | 7 | 4 | 3 |

当复制集内存活的成员数量不足大多数时，整个复制集将无法选举出主节点，此时**无法提供写服务**，这些节点都将处于**只读状态**。此外，如果希望避免平票结果的产生，**最好使用奇数个节点成员**，比如 3 个或 5 个。当然，在 MongoDB 复制集的实现中，对于平票问题已经提供了解决方案：

- 为选举定时器增加少量的**随机时间偏差**，这样避免各个节点在同一时刻发起选举，提高成功率
- 使用**仲裁者角色**，该角色不做数据复制，也不承担读写业务，仅仅用来投票

#### 常见选项

复制集节点有以下常见的选项配置

- 是否具有投票权（v 参数）：有则参与投票
- 优先级（priority）：优先级越高的节点越优先成为主节点，优先级为 0 的节点无法成为主节点，这个通常可用于将性能较高的或者距离较近的节点将其优先级设高一点
- 隐藏（hidden）：复制数据，但对应用不可见，而且优先级必须为 0，但是具备投票权，设置了 hidden 的节点往往是用于数据备份中心的
- 延迟（slaveDelay）：复制 n 秒之前的数据，保持与主节点的时间差，因为删除操作也会被同步到其他备节点的，所以如果不小心触发了删除操作，那将是一个非常危险的操作，而这个配置就是将数据同步延迟

### 自动故障转移

两个关键性问题：

1. 备节点如何感知到主节点已经发生故障了？

   心跳机制：

   一个影响检测机制的因素是心跳，在复制集组建完成之后，各成员节点会开启定时器，持续向其他成员发起心跳，这里涉及的参数为**heartbeatIntervalMillis**，即心跳间隔时间，默认值是**2s**。如果心跳成功，则会持续以 2s 的频率继续发送心跳；如果心跳失败，则会立即重试心跳，一直到心跳恢复成功

   选举超时检测：

   一次心跳检测失败并不会立即触发重新选举。实际上除了心跳，成员节点还会启动一个选举超时检测定时器，该定时器默认以**10s**的间隔执行，具体可以通过**electionTimeoutMillis**参数指定：

   心跳响应成功 → 则取消上一次的 electionTimeout 调度，并发起新一轮的 electionTimeout 定时检测

   心跳响应时间内不能成功 → 触发**electionTimeoutMillis**任务，开启选举

2. 如何降低故障转移对业务产生的影响？

   在复制集发生主备节点切换的情况下，会出现短暂的无主节点阶段，此时无法接受业务写操作。如果因为主节点故障导致的切换，则对于该节点的所有读写操作都会产生超时。如果使用 MongoDB 3.6 及以上版本的驱动，则可以通过开启**retryWrite**来降低影响。

   1. 如果想不丢数据重启复制集，更优雅的打开方式应该是这样的：
      1. 逐个重启复制集里所有的 Secondary 节点（主仍可接受数据）
      2. 对 Primary 发送 rs.stepDown()命令，等待 primary 降级为 Secondary(令新主提供服务)
      3. 重启降级后的 Primary

### 搭建 MongoDB 复制集

#### 创建数据目录

Linux/MacOS

```bash
mkdir -p /mongodb/db{1,2,3}
```

Windows 系统的话就在自定义盘下创建 3 个存储数据库信息的目录

#### 准备配置文件

当然各个实例在生产环境中都是应该要在不同的机器上进行部署的，而单机实验的话就需要注意各个 mongodb 实例的 port 要不一样，systemLog.path 要不一样和 storage.dbPath 要不一样

```mongodb
 # /data/db1/mongod.conf
 systemLog:
   destination: file
   path: /mongodb/db1/mongod.log/ or /mongodb/db2/mongod.log or /mongodb/db3/mongod.log # log path
   logAppend: true
 storage:
   dbPath: /mongodb/db1 or /mongodb/db2  or /mongodb/db3 # data directory
 net:
   bindIp: 0.0.0.0
   port: 28017/28018/28019 # port
 replication:
   replSetName: rs0
 processManagement:
   fork: true mongodb配置文件
```

#### 开启服务

接下来就可以通过配置文件开启 MongoDB 服务了

```bash
bin/mongod -f /mongodb/db1/mongodb.conf
bin/mongod -f /mongodb/db2/mongodb.conf
bin/mongod -f /mongodb/db3/mongodb.conf
```

#### 配置复制集

上面 👆 我们看了 3 个服务实例，但是他们之间是互不相关的，也不知道各自的存在，所以我们需要将他们关联到一起

有两种配置方式，首先我们得先通过 mongosh 进入主节点

```bash
mongosh --port 28017
```

##### 方式一

```mongodb
rs.initate()

rs.add("HOSTNAME:28018")
rs.add("HOSTNAME:28019")

```

可以通过 hostname -f linux 命令查看 hostname

##### 方式二

```mongodb
rs.initiate({

  _id: "rs0",
  members:[
    {
      _id : 0,
      host: "localhost:28017"
    },
    {
       _id : 1,
      host: "localhost:28018"
    },
     _id : 2,
      host: "localhost:28019"
  ]

})
```

这里的配置是将 id 为 0 的实例配置为主机，其他的为从机，完成了配置后，主机实例的身份会发生变化

```mongodb
rs0 [direct: other] test>
```

主从配置完成后，主机允许读写，从机是不允许读写的，从机想要进行读取，需要进行配置

```mongodb
rs0 [direct: primary] test> db.user.insertMany([{name:"test1"},{name:"test2"}])
{
  acknowledged: true,
  insertedIds: {
    '0': ObjectId('6723974161bdf98fff964033'),
    '1': ObjectId('6723974161bdf98fff964034')
  }
}
 主机写操作
```

```mongodb
rs0 [direct: primary] test> db.user.find()
[
  { _id: ObjectId('6723974161bdf98fff964033'), name: 'test1' },
  { _id: ObjectId('6723974161bdf98fff964034'), name: 'test2' }
] 从机读操作
```

当我们 mongosh 连接从机时，会发现从机的身份也发生了变化，并且当我们写数据的时候，系统会提示我们：说该节点不是主节点

```mongodb
rs0 [direct: secondary] test>

```

```mongodb
rs0 [direct: secondary] test> db.user.insert({name:"test3"})
DeprecationWarning: Collection.insert() is deprecated. Use insertOne, insertMany, or bulkWrite.
Uncaught:
MongoBulkWriteError[NotWritablePrimary]: not primary
 从节点写数据
```

当然如果我们想要从节点进行读取数据的操作，需要执行指令

```mongodb
rs.secondaryOk()
```

#### 查看状态

通过 rs.status()可以查看集群状态

```mongodb
rs0 [direct: primary] test> rs.status()
{
  set: 'rs0',
  date: ISODate('2024-10-31T14:52:40.489Z'),
  myState: 1,
  term: Long('1'),
  syncSourceHost: '',
  syncSourceId: -1,
  heartbeatIntervalMillis: Long('2000'),
  majorityVoteCount: 2,
  writeMajorityCount: 2,
  votingMembersCount: 3,
  writableVotingMembersCount: 3,
  optimes: {
    lastCommittedOpTime: { ts: Timestamp({ t: 1730386355, i: 1 }), t: Long('1') },
    lastCommittedWallTime: ISODate('2024-10-31T14:52:35.189Z'),
    readConcernMajorityOpTime: { ts: Timestamp({ t: 1730386355, i: 1 }), t: Long('1') },
    appliedOpTime: { ts: Timestamp({ t: 1730386355, i: 1 }), t: Long('1') },
    durableOpTime: { ts: Timestamp({ t: 1730386355, i: 1 }), t: Long('1') },
    lastAppliedWallTime: ISODate('2024-10-31T14:52:35.189Z'),
    lastDurableWallTime: ISODate('2024-10-31T14:52:35.189Z')
  },
  lastStableRecoveryTimestamp: Timestamp({ t: 1730386345, i: 1 }),
  electionCandidateMetrics: {
    lastElectionReason: 'electionTimeout',
    lastElectionDate: ISODate('2024-10-31T14:34:44.934Z'),
    electionTerm: Long('1'),
    lastCommittedOpTimeAtElection: { ts: Timestamp({ t: 1730385274, i: 1 }), t: Long('-1') },
    lastSeenOpTimeAtElection: { ts: Timestamp({ t: 1730385274, i: 1 }), t: Long('-1') },
    numVotesNeeded: 2,
    priorityAtElection: 1,
    electionTimeoutMillis: Long('10000'),
    numCatchUpOps: Long('0'),
    newTermStartDate: ISODate('2024-10-31T14:34:45.067Z'),
    wMajorityWriteAvailabilityDate: ISODate('2024-10-31T14:34:45.169Z')
  },
  members: [
    {
      _id: 0,
      name: 'localhost:28017',
      health: 1,
      state: 1,
      stateStr: 'PRIMARY',
      uptime: 82606,
      optime: { ts: Timestamp({ t: 1730386355, i: 1 }), t: Long('1') },
      optimeDate: ISODate('2024-10-31T14:52:35.000Z'),
      lastAppliedWallTime: ISODate('2024-10-31T14:52:35.189Z'),
      lastDurableWallTime: ISODate('2024-10-31T14:52:35.189Z'),
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      electionTime: Timestamp({ t: 1730385284, i: 1 }),
      electionDate: ISODate('2024-10-31T14:34:44.000Z'),
      configVersion: 1,
      configTerm: 1,
      self: true,
      lastHeartbeatMessage: ''
    },
    {
      _id: 1,
      name: 'localhost:28018',
      health: 1,
      state: 2,
      stateStr: 'SECONDARY',
      uptime: 1086,
      optime: { ts: Timestamp({ t: 1730386355, i: 1 }), t: Long('1') },
      optimeDurable: { ts: Timestamp({ t: 1730386355, i: 1 }), t: Long('1') },
      optimeDate: ISODate('2024-10-31T14:52:35.000Z'),
      optimeDurableDate: ISODate('2024-10-31T14:52:35.000Z'),
      lastAppliedWallTime: ISODate('2024-10-31T14:52:35.189Z'),
      lastDurableWallTime: ISODate('2024-10-31T14:52:35.189Z'),
      lastHeartbeat: ISODate('2024-10-31T14:52:39.625Z'),
      lastHeartbeatRecv: ISODate('2024-10-31T14:52:40.295Z'),
      pingMs: Long('0'),
      lastHeartbeatMessage: '',
      syncSourceHost: 'localhost:28017',
      syncSourceId: 0,
      infoMessage: '',
      configVersion: 1,
      configTerm: 1
    },
    {
      _id: 2,
      name: 'localhost:28019',
      health: 1,
      state: 2,
      stateStr: 'SECONDARY',
      uptime: 1085,
      optime: { ts: Timestamp({ t: 1730386355, i: 1 }), t: Long('1') },
      optimeDurable: { ts: Timestamp({ t: 1730386355, i: 1 }), t: Long('1') },
      optimeDate: ISODate('2024-10-31T14:52:35.000Z'),
      optimeDurableDate: ISODate('2024-10-31T14:52:35.000Z'),
      lastAppliedWallTime: ISODate('2024-10-31T14:52:35.189Z'),
      lastDurableWallTime: ISODate('2024-10-31T14:52:35.189Z'),
      lastHeartbeat: ISODate('2024-10-31T14:52:39.625Z'),
      lastHeartbeatRecv: ISODate('2024-10-31T14:52:39.160Z'),
      pingMs: Long('0'),
      lastHeartbeatMessage: '',
      syncSourceHost: 'localhost:28018',
      syncSourceId: 1,
      infoMessage: '',
      configVersion: 1,
      configTerm: 1
    }
  ],
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1730386355, i: 1 }),
    signature: {
      hash: Binary.createFromBase64('AAAAAAAAAAAAAAAAAAAAAAAAAAA=', 0),
      keyId: Long('0')
    }
  },
  operationTime: Timestamp({ t: 1730386355, i: 1 })
}

```

我们需要常关注的有

- health：成员是否健康，通过心跳检测
- state/stateStr:成员的状态，PRIMARY 表示主节点，SECONDARY 表示从节点，如果节点出现故障的话，会出现其他状态，比如 RECOVERY
- uptime：成员的启动时间
- optime、optimeDate：成员同步最后一条 oplog 的时间
- optimeDurable、optimeDurableDate：成员同步最后一条 oplog 持久化的时间
- pingMs：成员与当前节点的 ping 时延
- syncingTo：成员的同步来源

当然我们还可以通过 db.isMaster()来查看当前节点的角色，配置信息包含了复制集的成员列表，Primary，协议等相关配置信息

```mongodb
rs0 [direct: primary] test> db.isMaster()
{
  topologyVersion: {
    processId: ObjectId('6722570abb0dee2cb80de1ea'),
    counter: Long('6')
  },
  hosts: [ 'localhost:28017', 'localhost:28018', 'localhost:28019' ],
  setName: 'rs0',
  setVersion: 1,
  ismaster: true,
  secondary: false,
  primary: 'localhost:28017',
  me: 'localhost:28017',
  electionId: ObjectId('7fffffff0000000000000001'),
  lastWrite: {
    opTime: { ts: Timestamp({ t: 1730386645, i: 1 }), t: Long('1') },
    lastWriteDate: ISODate('2024-10-31T14:57:25.000Z'),
    majorityOpTime: { ts: Timestamp({ t: 1730386645, i: 1 }), t: Long('1') },
    majorityWriteDate: ISODate('2024-10-31T14:57:25.000Z')
  },
  maxBsonObjectSize: 16777216,
  maxMessageSizeBytes: 48000000,
  maxWriteBatchSize: 100000,
  localTime: ISODate('2024-10-31T14:57:33.568Z'),
  logicalSessionTimeoutMinutes: 30,
  connectionId: 35,
  minWireVersion: 0,
  maxWireVersion: 17,
  readOnly: false,
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1730386645, i: 1 }),
    signature: {
      hash: Binary.createFromBase64('AAAAAAAAAAAAAAAAAAAAAAAAAAA=', 0),
      keyId: Long('0')
    }
  },
  operationTime: Timestamp({ t: 1730386645, i: 1 }),
  isWritablePrimary: true
}

```

#### 其他常用指令

| 命令                                   | 功能描述                                                       |
| -------------------------------------- | -------------------------------------------------------------- |
| \`rs.add()\`                           | 将节点添加到副本集                                             |
| \`rs.addArb()\`                        | 将仲裁节点添加到副本集                                         |
| \`rs.conf()\`                          | 返回副本集配置文档                                             |
| \`rs.freeze()\`                        | 阻止当前节点在一段时间内寻求选举为主节点                       |
| \`rs.help()\`                          | 返回副本集函数的基本帮助文本                                   |
| \`rs.initiate()\`                      | 初始化新的副本集                                               |
| \`rs.printReplicationInfo()\`          | 从主节点的角度打印副本集状态的格式化报告                       |
| \`rs.printSecondaryReplicationInfo()\` | 从从节点的角度打印副本集状态的格式化报告                       |
| \`rs.reconfig()\`                      | 通过应用新的副本集配置对象来重新配置副本集                     |
| \`rs.remove()\`                        | 从副本集中删除节点                                             |
| \`rs.status()\`                        | 返回包含副本集状态信息的文档                                   |
| \`rs.stepDown()\`                      | 导致当前的 主节点变为强制选举的从节点                          |
| \`rs.syncFrom()\`                      | 设置该副本集节点与哪个节点进行同步，复写默认的同步目标选择逻辑 |
| \`applyOps\`                           | 将 oplog 条目应用于当前数据集的内部命令                        |
| \`hello\`                              | 显示该节点在副本集中的角色信息，包括是否为主副本               |
| \`replSetAbortPrimaryCatchUp\`         | 强制当选的主节点放弃同步（追赶）过程，然后完成到主节点的过渡   |
| \`replSetFreeze\`                      | 阻止当前节点在一段时间内寻求选举为主节点                       |
| \`replSetGetConfig\`                   | 返回副本集的配置对象                                           |
| \`replSetGetStatus\`                   | 返回报告副本集状态的文档                                       |
| \`replSetInitiate\`                    | 初始化新的副本集                                               |
| \`replSetMaintenance\`                 | 启用或禁用维护模式，该模式使从节点处于 RECOVERING 状态         |
| \`replSetReconfig\`                    | 将新配置应用于现有副本集                                       |
| \`replSetResizeOplog\`                 | 动态调整 oplog 大小                                            |

#### 复制集的连接方式

当我们想要连接 Mongodb 的复制集时，一种方式是如果我们知道主节点可以直接连接主节点来完成读写操作，但是如果主节点发生了故障，主节点便会切换，这个时候就无法通过访问旧的主节点来访问复制集了

```bash
mongosh --port 28017
```

所以强烈推荐第二种，通过高可用 Uri 的方式连接 MongoDB，当主节点故障切换后，MongoDB 引擎可自动感知并将流量路由到新的主节点

```bash
mongosh mongodb://localhost:28017,localhost:28018,localhost:28019/admin?replicaSet=rs0
```

### 安全认证

作为数据库，安全是首要重要的，所以如何为复制集配安全认证呢？

#### 连接主服务器

首先连接到主服务器 mongodb，使用 admin

```bash
bin/mongosh --port 28017
use admin
```

#### 创建用户

```mongodb

 db.createUser( {
   user: "tang",
   pwd: "tang",
   roles: [
     { role: "clusterAdmin", db: "admin" } ,
     { role: "userAdminAnyDatabase", db: "admin"},
     { role: "userAdminAnyDatabase", db: "admin"},
     { role: "readWriteAnyDatabase", db: "admin"}]
    })

 创建用户
```

```mongodb
//响应结果
{
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1730543556, i: 4 }),
    signature: {
      hash: Binary.createFromBase64('AAAAAAAAAAAAAAAAAAAAAAAAAAA=', 0),
      keyId: Long('0')
    }
  },
  operationTime: Timestamp({ t: 1730543556, i: 4 })
}

```

#### 停止服务

```bash
bin/mongod  -f /mongodb/db1/mongo.conf --shutdown
bin/mongod  -f /mongodb/db2/mongo.conf --shutdown
bin/mongod  -f /mongodb/db3/mongo.conf --shutdown

```

#### 创建 keyFile 文件

keyFile 是集群之间的安全认证，增加安全认证机制 KeyFile，开启 keyFile 认证就默认开启了 auth 认证

基于 openssl 创建一个 64 位密钥存储到 mongo.key 文件内

```bash
openssl rand -base64 756 > /mongodb/db1/mongo.key
```

调整 key 文件为 600 权限，即只有文件所有者可以读写

```bash
openssl rand -base64 756 > /mongodb/db1/mongo.key
```

然后就可以通过 keyFile 来开启 mongodb 服务了

#### 启动服务

```bash
 bin/mongod  -f /mongodb/db1/mongo.conf --keyFile /mongodb/db1/mongo.key
 bin/mongod  -f /mongodb/db2/mongo.conf --keyFile /mongodb/db1/mongo.key
 bin/mongod  -f /mongodb/db3/mongo.conf --keyFile /mongodb/db1/mongo.key
```

#### 测试连接

如果直接连接服务器的话，访问数据会被拒绝

```bash
rs0 [direct: primary] admin> db.user.find()
MongoServerError[Unauthorized]: command find requires authentication

```

基于用户名密码登录

```bash
mongosh --port 28017 -u tang -p tang --authenticationDatabase=admin
```
